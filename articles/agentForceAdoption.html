<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agentforce Adoption</title>
    <link rel="stylesheet" href="../template/articleTemplate.css">
</head>
<body>
    <header>
        <h1>Agentforce Adoption</h1>
    </header>
    <main>
        <article>
            <p>The ever-evolving artificial intelligence ecosystem offers no shortage of options. Choosing the right one and getting people to adopt it proves to be an arduous task. As an admin, I've been experimenting with models such as ChatGPT 4 and Claude 3.7 on my desktop, but I want a more secure and productive way of utilizing them. These powerful tools have the potential to transform how we process and summarize complex information, particularly in grant-making for social and economic research. Despite their promise, I've encountered several barriers.</p>
            <p>As colleagues experiment with AI models in haphazard ways, the risk of exposing sensitive data becomes a critical concern. The need for robust data anonymization techniques is paramount to ensure that personally identifiable information is protected. Furthermore, to maximize the return on investment in AI technologies, it is essential to target substantial, time-consuming tasks that can significantly benefit from automation.</p>
            <h2>Your Colleagues Are Creating Shadow Systems</h2>
            <p>The first barrier is not that people are averse to using AI models; it's that they are already using them in their own haphazard ways. If you work in a tech role and use any popular database, you will know there is a great deal of talk about using these models. If you glanced at the screens of your colleagues, you will no doubt see that they are using search engines that incorporate chatbots. Google Search uses Gemini, Bing uses OpenAI—these are now part of how every user searches. If I asked my colleagues directly, they already know how to use Microsoft Co-Pilot (ChatGPT by OpenAI) or might have found another option such as Le Chat or DeepSeek. End-users, managers, and directors outside of technology are demanding these services both formally and informally.</p>
            <p>The risk of these services is that one might be tempted to copy and paste critical data into them. Would it be anonymized automatically by an algorithm? Is the data exposing individual information? I don't necessarily know the exact answers to these questions, and I think examining the inner workings of AI models deserves its own series.</p>
            <p>My assumption is that models trained on billions of parameters are unlikely to be influenced by such small quantities of data. When fine-tuning with a small dataset, the model may not capture the breadth of patterns and nuances present in the language, leading to poorer performance on unseen data. For this reason, I cannot understand how individuals would ever be identified unless their names were indexed in search engines. Even so, one must strive to find a way of anonymizing data to be confident that data is not being leaked.</p>
            <p>Repeated mentions of a person or institution might lead to a clear pattern, though this might require thousands of repeated prompts and observations across several datasets. I don't believe that AI models have the human-like memory that some commentators seem to think they have. If you want to test this, start a prompt with any service, press submit, start a new prompt, and then try to reference the previous conversation. Almost every generative AI service I have used acts as though the second prompt is completely different from the first and appears to have no recollection of the previous conversation.</p>
            <p>If it does have a convincing response, it is because I fed it a very large prompt template that asked for the history of the record or an activity timeline. In other words, it knows the history because you fed it the history, not because it remembers what you wrote earlier. One colleague keeps a series of templates or a cheat sheet to make prompts quickly. This was ingenious, but surely there is a better way that doesn't rely on the user having to remember to remove sensitive data.</p>
            <p>I have tried this with Claude 3.7, ChatGPT 4, Le Chat, and DeepSeek. It seems that, for the time being, this is an inevitable feature of the models. Though newer versions may well come to have better in-chat memory or even be able to access previous chats, for now, I assume that every chat is brand new. For that reason, I must rewrite prompts to reflect this, including removing sensitive fields.</p>
            <p>I think it is great that white-collar workers are open-minded to the possibilities that these tools could unlock. However, would it be safer, more convenient, and easier to connect the Large Language Model (LLM) to Salesforce and offer people a method of using a prompt in an efficient manner?</p>
            <h2>Prompts Must Use Masked Data</h2>
            <p>The second barrier is how to correctly mask the data and stop users and automations from sending personally identifiable data to the LLM. The easier way is to have the wisdom not to include sensitive fields in the first place. I don't use financial data or addresses for example but there may well be genuine use cases for this in other contexts. I might be showing an abundance of caution, and for that reason, I could safely use these fields. I think that there is plenty to be gained from using operational data that doesn't directly refer to any individual customer, and that this is the sweet spot that minimizes risk while maximizing gains.</p>
            <p>If you must use personally identifiable data, there is a service where you can cover or hide the sensitive data you do not want to share, such as the Einstein Privacy Layer. This intervenes between the action to generate a prompt using a prompt template and the response being sent to an LLM. To my mind, it is just a fancy term for data protection. It reassures the CRM owner that their data is not being sent to the LLM completely exposed and visible for all the artificial neurons to see.</p>
            <p>Data masking works like merge fields in reverse: a piece of data is covered up with a placeholder value and modified so it can no longer be traced back to any person. The customer name becomes {contact name}, the account becomes {account name}, and so on. The LLM is prompted to calculate a response in relation to these placeholders. I think that this problem was solved long before generative AI models became popular, and one can rely on the accuracy of this approach. A prompt and the associated data is a jumble of characters, but sensitive data often has a very clear structure, making it easy to find and mask.</p>
            <h2>Write Longer Prompts</h2>
            <p>The third barrier is the need to focus on substantial tasks that your colleagues are spending a lot of time on to generate good returns on your investment. Prioritizing tasks is a costly and lengthy process that requires inputs from multiple teams.</p>
            <p>You need your business analyst hat on, and you will need to focus on lengthy tasks that take 20-40 minutes per account. My colleagues write 500-word summaries of 5000-word-long applications plus a few thousand more for reviews. These tasks could be completed relatively quickly if a draft was provided by the AI.</p>
            <p>Anything shorter than this probably doesn't generate healthy returns, and you are not saving enough time per person per team. Your AI licenses are being wasted if they are not being used to target high-value work. Of course, you could argue to the contrary and say that high-volume, low-complexity tasks could easily be automated to allow for bigger tasks to be completed by humans. My gut instinct is that if it really is that small, you've probably spent more time automating it than the benefit received. Is this called over-automation or time-wasting?</p>
            <p>Prompt writing and engineering are different things. The writing part takes place in the text window, but the engineering part includes gathering and anonymizing the data, retrieving it quickly, and combining it with the instructions inside the prompt. The prompt can then be sent to the LLM of your choice. This is equal parts art and science, and the only good advice I have is to write long prompts of 600 words that retrieve multiple fields rather than slimmer 200-word prompts that are more precise.</p>
            <p>Initially, I created smaller prompts that combined five input text fields into one output text field. I repeated this process for each different part of a summary. The results were appalling. Colleagues did not like the vague, repetitive responses and did not want to continue using their assistant if the work was this bad. The bullet point summary did not offer a clear understanding of either the institution being summarized or the topic being investigated.</p>
            <p>I rewrote four or five prompts into one longer prompt and got a much better response from my colleagues. There was something about the longer, more complex prompt with a greater number of fields that performed better. I can't fathom why; the data hasn't changed, and the prompt text did not change. I adjusted word limits but nothing else. Perhaps it is simply offering more context in one prompt rather than the smaller prompts I had before.</p>
            <p>Either way, this was better received, and the ultimate test of the success of a prompt is that your colleagues want to use it. They hit the button, take the summary, and plow on with their work. Done—I know that I have invested time and money wisely and have created something valuable.</p>
            <h2>Conclusion</h2>
            <p>In conclusion, the adoption of AI models via Salesforce for summarizing long text fields presents both opportunities and challenges. The barriers to adoption—shadow systems, data masking, and the focus on high-impact tasks—highlight the need for a strategic approach to integrating these powerful tools. By addressing the proliferation of unregulated AI usage, organizations can mitigate the risks associated with data exposure and ensure compliance with privacy standards. Implementing robust data masking techniques is crucial to protect sensitive information and build trust in AI-driven processes.</p>
            <p>Moreover, prioritizing substantial tasks that offer significant time savings can maximize the return on investment in AI technologies. The experience of creating longer, more complex prompts that yield better results underscores the importance of context and comprehensiveness in AI interactions.</p>
            <p>Ultimately, the successful integration of AI models into workflows requires a balance of innovation, caution, and strategic planning. By overcoming the identified barriers, organizations can unlock the full potential of AI to enhance productivity, improve accuracy, and drive innovation.</p>
        </article>
    </main>
    <footer>
        <p>Copyright &copy; 2025</p>
    </footer>
</body>
</html>

